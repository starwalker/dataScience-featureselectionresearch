{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732a42d6",
   "metadata": {},
   "source": [
    "## Hosptial Mortality Classifcation\n",
    "this notebookes creates classifers that predict probablity that a patient died in the hospital based on lab values. It uses Phyisio Mimic III as a data source and uses the python evalML to evaluate classifers. m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7ea522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-n N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f \/home\/bidev/.local/share/jupyter/runtime/kernel-0100ce7d-1d39-41c0-baf3-3aeb04a3df83.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate,  StratifiedKFold\n",
    "from sklearn.metrics import *\n",
    "import argparse\n",
    "from evalml.automl import AutoMLSearch\n",
    "import evalml\n",
    "import os\n",
    "import re\n",
    "import mlflow\n",
    "from evalml.model_understanding.prediction_explanations import explain_predictions\n",
    "from mlflow.models.signature import infer_signature\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "MAX_MEMORY = \"32g\"\n",
    "data_dir = os.getenv('PHYSIO_HOME')\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('-n',default=100, type=int,  help=\"integer Number of features to select\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# sets the number of features to section by frequency\n",
    "n_features = args.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a9bf6",
   "metadata": {},
   "source": [
    "#### Data Loading\n",
    "Data is loaded from Phyiso MimiIII amd saved as paquet to pyspark data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532a1cf",
   "metadata": {},
   "source": [
    "#### Data Egneineering \n",
    "creates a features data frame using max and min lab values during hospital stays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads all the csvs and writes them to parquet filesspark = SparkSession.builder \\\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HostpitalMortalityClassifier\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "LABEVENTS =  spark.read.parquet(data_dir + '/LABEVENTS.parquet')\n",
    "D_LABITEMS =  spark.read.parquet(data_dir + '/D_LABITEMS.parquet')\n",
    "ADMISSIONS =   spark.read.parquet(data_dir + '/ADMISSIONS.parquet')\n",
    "\n",
    "\n",
    "# sets the number of features to section by frequency\n",
    "n_features = 100\n",
    "\n",
    "# gets the top n_features most frequent features \n",
    "top_features = LABEVENTS\\\n",
    "                .join(D_LABITEMS, on = 'ITEMID', how='inner')\\\n",
    "                .dropna(subset=['VALUENUM'])\\\n",
    "                .groupby('LABEL')\\\n",
    "                .count().sort('count', ascending=False)\\\n",
    "                .limit(n_features).drop('count')\n",
    "\n",
    "\n",
    "## Data Transformations \n",
    "## gets the max and min value from the top n_features\n",
    "## groups by hospital admit id\n",
    "## creates a flag where the patient died \"Expired\" in the hosptial                                        \n",
    "data = LABEVENTS\\\n",
    ".join(D_LABITEMS, on = 'ITEMID', how='inner')\\\n",
    ".join(top_features, on='label', how='inner')\\\n",
    ".dropna(subset=['VALUENUM'])\\\n",
    ".groupby('HADM_ID')\\\n",
    ".pivot('LABEL')\\\n",
    ".agg(max('VALUENUM').alias('max'), min('VALUENUM').alias('min'))\\\n",
    ".join(ADMISSIONS.select('HADM_ID', col('HOSPITAL_EXPIRE_FLAG').alias('label')), on='HADM_ID', how='inner')\\\n",
    ".filter('label in (0,1)')\n",
    "\n",
    "## data Extraction to Pandas\n",
    "df = data.toPandas().set_index('HADM_ID')\n",
    "\n",
    "## create arrays for training model \n",
    "y = df.loc[:, 'label'].values\n",
    "X = df.drop('label', axis=1).values\n",
    "\n",
    "n_rows = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "feature_names_all = np.array(list(df.drop('label', axis=1).columns))\n",
    "label_prob = y.mean()\n",
    "print(F' n_rows: {n_rows}, n_features: {n_features}, label_prob {np.round(label_prob , 3)}')\n",
    "print(F'features: {feature_names_all}')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2521b24",
   "metadata": {},
   "source": [
    "#### Basic Data Statics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48138e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats_path = 'data_stats.csv'\n",
    "data_stats = df.describe()\n",
    "data_stats.to_csv(data_stats_path)\n",
    "data_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6754a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b4546",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "Data splitting via Statified Shuffle Split\n",
    "\n",
    "#### Feature Selection pyImpetous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dfc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "splitter = StratifiedKFold(shuffle=True)\n",
    "train_index, test_index = next(splitter.split(X, y))\n",
    "imputer = SimpleImputer()\n",
    "selector = RFE(DecisionTreeClassifier(min_samples_leaf=5, max_depth=10) ,step=5, n_features_to_select=n_features)\n",
    "\n",
    "#featuer selection via recussive feature elemination \n",
    "selector= selector.fit(pd.DataFrame(imputer.fit_transform(X[train_index, :]), columns=feature_names_all).fillna(0), \n",
    "                               y[train_index])\n",
    "\n",
    "support_index = selector.get_support()\n",
    "feature_names = feature_names_all[support_index]\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "X_train = imputer.fit_transform(df.iloc[train_index, :].loc[:, feature_names])\n",
    "X_test = imputer.fit_transform(df.iloc[test_index, :].loc[:, feature_names])\n",
    "y_train = y[train_index]\n",
    "y_test = y[test_index]\n",
    "\n",
    "\n",
    "print(F'Selected Feature Names {feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "estimator = LogisticRegression(max_iter=5000)\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a5972",
   "metadata": {},
   "source": [
    "#### Modeliing Fitting Using AutoML\n",
    "Searchs through models to find best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "model = Pipeline(steps=[('imp', imputer),('classifier', estimator)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa2c0f",
   "metadata": {},
   "source": [
    "#### Model Performance\n",
    "Calcuates Model Peformace on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02627db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts the test data\n",
    "test_preds = estimator.predict_proba(X_test)[:, 1]\n",
    "test_pred_labels = estimator.predict(X_test)\n",
    "\n",
    "# calcuates metrics on test data\n",
    "test_f1 = f1_score(y_test, test_pred_labels)\n",
    "test_acc_balanced = balanced_accuracy_score(y_test, test_pred_labels)\n",
    "test_acc = accuracy_score(y_test, test_pred_labels)\n",
    "test_precision = precision_score(y_test, test_pred_labels)\n",
    "test_recall = recall_score(y_test, test_pred_labels)\n",
    "test_auc_score = roc_auc_score(y[test_index], test_preds)\n",
    "print(F'roc_auc_score: {test_auc_score } on test')\n",
    "\n",
    "## predicts the training data \n",
    "train_preds = model.predict_proba(X_train)[:, 1]\n",
    "train_pred_labels = model.predict(X_train)\n",
    "\n",
    "# calculates metrics on training data \n",
    "train_f1 = f1_score(y_train, train_pred_labels)\n",
    "train_acc_balanced = balanced_accuracy_score(y_train, train_pred_labels)\n",
    "train_acc = accuracy_score(y_train, train_pred_labels)\n",
    "train_precision = precision_score(y_train, train_pred_labels)\n",
    "train_recall = recall_score(y_train, train_pred_labels)\n",
    "train_auc_score = roc_auc_score(y_train, train_preds)\n",
    "print(F'roc_auc_score: {train_auc_score} on train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets params Artifacts for logging mlflow model\n",
    "n_cases = np.sum(y == 1)\n",
    "n_controls = np.sum(y == 0)\n",
    "n_train_obs = X_train.shape[0]\n",
    "n_test_obs = X_test.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "train_label_prob = y_train.mean()\n",
    "test_label_prob = y_test.mean()\n",
    "model_type = type(estimator)\n",
    "split_type = type(splitter)\n",
    "input_example = pd.DataFrame(X_train[:10, :], columns=feature_names).fillna(0)\n",
    "signature = infer_signature(input_example , model.predict_proba(input_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474481a9",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "save feature importance to a dictionary for later logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a28c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.Series(estimator.coef_[0], index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "# dumps feature importance to a dictionary for logging as an artifact\n",
    "imp_dict = imp.to_dict()\n",
    "imp_json_path = 'feature_importance.json'\n",
    "with open(imp_json_path, 'w') as f:\n",
    "    json.dump(imp_dict,f)\n",
    "\n",
    "imp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669a0c3",
   "metadata": {},
   "source": [
    "#### Model Tracking\n",
    "Uses an mlflow tracking server to save the model, parameters and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path = 'Model'\n",
    "data_grain = 'HADM_ID'\n",
    "label_name = 'HOSPITAL_EXPIRE_FLAG'\n",
    "data_source = 'PhysioMimicIII'\n",
    "run_name = 'rfe_log_reg'\n",
    "tracking_uri = \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name, experiment_id=1) as run:\n",
    "    \n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    artifact_uri = mlflow.get_artifact_uri()\n",
    "    print(\"Tracking uri: {}\".format(tracking_uri))\n",
    "    print(\"Artifact uri: {}\".format(artifact_uri))\n",
    "    mlflow.sklearn.log_model(model,\n",
    "                         artifact_path=artifact_path, \n",
    "                         signature=signature,\n",
    "                         input_example=input_example\n",
    "                        )\n",
    "    mlflow.log_artifact(imp_json_path)\n",
    "    mlflow.log_artifact(data_stats_path)\n",
    "    mlflow.log_param('data_source', data_source)\n",
    "    mlflow.log_param('label_name', label_name)\n",
    "    mlflow.log_param('data_grain', data_grain)\n",
    "    mlflow.log_param('n_cases', n_cases)\n",
    "    mlflow.log_param('n_controls', n_controls)\n",
    "    mlflow.log_param('n_train_obs', n_train_obs)\n",
    "    mlflow.log_param('n_test_obs', n_test_obs)\n",
    "    mlflow.log_param('n_features', n_features)\n",
    "    mlflow.log_param('train_label_prob', train_label_prob)\n",
    "    mlflow.log_param('test_label_prob', test_label_prob)\n",
    "    mlflow.log_param('desc', 'rfe_selector_with_log_reg')\n",
    "    mlflow.log_param('model_type',model_type)\n",
    "    mlflow.log_param('split_type',split_type)\n",
    "    mlflow.log_param('feature_selection', type(selector))\n",
    "    mlflow.log_metric('train_f1', train_f1)\n",
    "    mlflow.log_metric('train_acc_balanced', train_acc_balanced)\n",
    "    mlflow.log_metric('train_acc', train_acc)\n",
    "    mlflow.log_metric('train_precision', train_precision)\n",
    "    mlflow.log_metric('train_recall', train_recall)\n",
    "    mlflow.log_metric('train_auc_score', train_auc_score)\n",
    "    mlflow.log_metric('test_f1', test_f1)\n",
    "    mlflow.log_metric('test_acc_balanced', test_acc_balanced)\n",
    "    mlflow.log_metric('test_acc', test_acc)\n",
    "    mlflow.log_metric('test_precision', test_precision)\n",
    "    mlflow.log_metric('test_recall', test_recall)\n",
    "    mlflow.log_metric('test_auc_score', test_auc_score)\n",
    "    mlflow.log_param('features', '|'.join(imp.index))\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id \n",
    "    mlflow.end_run()\n",
    "    print(F'logging experiment_id: \"{experiment_id}\" run_id :\"{run_id}\" completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f5a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db431b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
